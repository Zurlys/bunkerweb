{
  "id": "robotstxt",
  "name": "Robots.txt",
  "description": "Manage the robots.txt file. Standard for controlling web crawler access to your site.",
  "version": "1.0",
  "stream": "yes",
  "settings": {
    "USE_ROBOTSTXT": {
      "context": "multisite",
      "default": "no",
      "help": "Enable robots.txt file.",
      "id": "use-robotstxt",
      "label": "Use robots.txt",
      "regex": "^(yes|no)$",
      "type": "check"
    },
    "ROBOTSTXT_COMMUNITY_LISTS": {
      "context": "multisite",
      "default": "",
      "help": "Select community-maintained lists to include in robots.txt.",
      "id": "robotstxt-community-lists",
      "label": "Community robots.txt lists",
      "regex": "^.*$",
      "type": "multiselect",
      "multiselect": [
        {
          "id": "ai-robots-txt",
          "label": "This list contains AI-related crawlers of all types, regardless of purpose.",
          "value": "https://raw.githubusercontent.com/ai-robots-txt/ai.robots.txt/refs/heads/main/robots.txt"
        },
        {
          "id": "robots-disallowed",
          "label": "The RobotsDisallowed project is a harvest of the robots.txt disallowed directories of the world's top websites - specifically those of the Alexa 100K and the Majestic 100K.",
          "value": "https://raw.githubusercontent.com/danielmiessler/RobotsDisallowed/refs/heads/master/curated.txt"
        }
      ]
    },
    "ROBOTSTXT_URLS": {
      "context": "multisite",
      "default": "",
      "help": "List of URLs, separated with spaces, containing robots.txt rules. Also supports file:// URLs and and auth basic using http://user:pass@url scheme.",
      "id": "robotstxt-urls",
      "label": "Robots.txt URLs",
      "regex": "^( *((https?:\\/\\/|file:\\\/\\/\\/)[\\-\\w@:%.+~#=]+[\\-\\w\\(\\)!@:%+.~#?&\\/=$]*) *)*$",
      "type": "multivalue",
      "separator": " "
    },
    "ROBOTSTXT_RULE": {
      "context": "multisite",
      "default": "",
      "help": "Rules for robots.txt (one per line).",
      "id": "robotstxt-rule",
      "label": "Robots.txt rule",
      "regex": "^.*$",
      "type": "text",
      "multiple": "robotstxt-rule"
    },
    "ROBOTSTXT_SITEMAP": {
      "context": "multisite",
      "default": "",
      "help": "Sitemap URL(s) to include in robots.txt.",
      "id": "robotstxt-sitemap",
      "label": "Robots.txt sitemap",
      "regex": "^(https?://\\S+)?$",
      "type": "text",
      "multiple": "robotstxt-sitemap"
    },
    "ROBOTSTXT_IGNORE_RULES": {
      "context": "multisite",
      "default": "",
      "help": "Rule (PCRE regex) to ignore. Can be specified multiple times.",
      "id": "robotstxt-ignore-rules",
      "label": "Ignore rule",
      "regex": "^.*$",
      "type": "text",
      "multiple": "robotstxt-ignore-rules"
    }
  },
  "jobs": [
    {
      "name": "robots-txt-download",
      "file": "robots-txt-download.py",
      "every": "hour",
      "reload": true,
      "async": true
    }
  ]
}
