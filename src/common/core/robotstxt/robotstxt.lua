local class = require "middleclass"
local plugin = require "bunkerweb.plugin"
local utils = require "bunkerweb.utils"

local ngx = ngx
local ERR = ngx.ERR
local get_phase = ngx.get_phase
local get_multiple_variables = utils.get_multiple_variables

local robotstxt = class("robotstxt", plugin)

function robotstxt:initialize(ctx)
	plugin.initialize(self, "robotstxt", ctx)
	if get_phase() ~= "init" then
		local robot_policies, err = self.datastore:get("plugin_robotstxt_policies", true)
		if not robot_policies then
			self.logger:log(ERR, err)
			return
		end
		self.robot_policies = {
			["rule"] = {},
			["sitemap"] = {},
		}
		if robot_policies.global then
			for k, v in pairs(robot_policies.global) do
				self.robot_policies[k] = v
			end
		end
		if robot_policies[self.ctx.bw.server_name] then
			for k, v in pairs(robot_policies[self.ctx.bw.server_name]) do
				self.robot_policies[k] = v
			end
		end
	end
end

function robotstxt:init()
	local variables, err = get_multiple_variables({
		"ROBOTSTXT_RULE",
		"ROBOTSTXT_SITEMAP",
	})
	if variables == nil then
		return self:ret(false, err)
	end
	local data = {}
	local key
	for srv, vars in pairs(variables) do
		if data[srv] == nil then
			data[srv] = {
				["rule"] = {},
				["sitemap"] = {},
			}
		end
		for var, value in pairs(vars) do
			if value ~= "" then
				key = string.lower(string.gsub(string.gsub(var, "^ROBOTSTXT_", ""), "_%d+$", ""))
				data[srv][key][#data[srv][key] + 1] = value
			end
		end
	end
	local ok
	ok, err = self.datastore:set("plugin_robotstxt_policies", data, nil, true)
	if not ok then
		return self:ret(false, err)
	end
	return self:ret(true, "successfully loaded robots.txt policies")
end

local function sanitize_rules(rules)
	local seen = {}
	local sanitized = {}
	local has_user_agent = false
	for _, rule in ipairs(rules) do
		local trimmed = rule:match("^%s*(.-)%s*$")
		if not seen[trimmed] then
			seen[trimmed] = true
			table.insert(sanitized, trimmed)
			if trimmed:lower():find("^user%-agent:") then
				has_user_agent = true
			end
		end
	end
	if not has_user_agent then
		table.insert(sanitized, 1, "User-agent: *")
	end
	return sanitized
end

local function sanitize_sitemaps(sitemaps)
	local seen = {}
	local sanitized = {}
	for _, url in ipairs(sitemaps) do
		local trimmed = url:match("^%s*(.-)%s*$")
		if trimmed ~= "" then
			-- Force HTTPS for sitemaps
			if trimmed:match("^http://") then
				trimmed = trimmed:gsub("^http://", "https://")
			end
			if not seen[trimmed] then
				seen[trimmed] = true
				table.insert(sanitized, trimmed)
			end
		end
	end
	return sanitized
end

function robotstxt:access()
	if self.ctx.bw.uri == "/robots.txt" and self.variables["USE_ROBOTSTXT"] ~= "no" then
		-- If no rules are set, use the default rule
		if self.robot_policies["rule"] == nil or #self.robot_policies["rule"] == 0 then
			self.robot_policies["rule"] = { "User-agent: *", "Disallow: /" }
		end
		local data = {}
		for k, v in pairs(self.robot_policies) do
			data[k] = v
		end
		data["rule"] = sanitize_rules(data["rule"])
		data["sitemap"] = sanitize_sitemaps(data["sitemap"])
		data["header"] = "# robots.txt generated by BunkerWeb (https://bunkerweb.io)"
		ngx.header["Content-Type"] = "text/plain; charset=utf-8"
		local output = {}
		if data["header"] then
			table.insert(output, data["header"])
		end
		for _, rule in ipairs(data["rule"]) do
			table.insert(output, rule)
		end
		for _, sitemap in ipairs(data["sitemap"]) do
			table.insert(output, "Sitemap: " .. sitemap)
		end
		ngx.say(table.concat(output, "\n"))
		return ngx.exit(ngx.HTTP_OK)
	end
	return self:ret(true, "success")
end

return robotstxt
